processing images... 
  0%|          | 0/8 [00:00<?, ?it/s]
 12%|█▎        | 1/8 [00:00<00:00,  8.61it/s]/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 4.79 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv2d(input, weight, bias, self.stride,
/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 9.57 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv2d(input, weight, bias, self.stride,
/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 7.95 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv2d(input, weight, bias, self.stride,
/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 11.92 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv2d(input, weight, bias, self.stride,

 25%|██▌       | 2/8 [00:09<00:34,  5.67s/it]
 38%|███▊      | 3/8 [00:19<00:37,  7.45s/it]
 50%|█████     | 4/8 [00:29<00:33,  8.37s/it]
 62%|██████▎   | 5/8 [00:38<00:26,  8.82s/it]
 75%|███████▌  | 6/8 [00:48<00:18,  9.14s/it]
 88%|████████▊ | 7/8 [00:58<00:09,  9.32s/it]
100%|██████████| 8/8 [01:07<00:00,  9.42s/it]
100%|██████████| 8/8 [01:07<00:00,  8.46s/it]
Done
computing cossim... 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  5.16it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.11it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.73it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
csv file saved at: ./temp
Done
Defender initialized with threshold=-0.0005046081542968749
filtered images are saved to ./output/img
processed 100 images, 100 texts in 120.34s
Loading MiniGPT-4 models...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading Q-Former
Loading Q-Former Done
Load MiniGPT-4 Checkpoint: /home/xuyue/QXYtemp/MLM/minigpt4/prerained_minigpt4_7b.pth
Done

generating response:   0%|          | 0/100 [00:00<?, ?it/s]
generating response:   1%|          | 1/100 [00:36<1:00:53, 36.91s/it]
generating response:   2%|▏         | 2/100 [01:13<59:49, 36.63s/it]  
generating response:   3%|▎         | 3/100 [01:49<59:08, 36.58s/it]
generating response:   4%|▍         | 4/100 [02:25<58:05, 36.31s/it]
generating response:   5%|▌         | 5/100 [03:01<57:16, 36.18s/it]
generating response:   6%|▌         | 6/100 [03:37<56:42, 36.20s/it]
generating response:   7%|▋         | 7/100 [04:14<56:28, 36.44s/it]
generating response:   8%|▊         | 8/100 [04:51<55:50, 36.42s/it]
generating response:   9%|▉         | 9/100 [05:28<55:35, 36.65s/it]
generating response:  10%|█         | 10/100 [06:03<54:26, 36.29s/it]
generating response:  11%|█         | 11/100 [06:41<54:15, 36.58s/it]
generating response:  12%|█▏        | 12/100 [07:17<53:36, 36.55s/it]
generating response:  13%|█▎        | 13/100 [07:54<53:03, 36.59s/it]
generating response:  14%|█▍        | 14/100 [08:30<52:26, 36.58s/it]
generating response:  15%|█▌        | 15/100 [09:07<51:59, 36.70s/it]
generating response:  16%|█▌        | 16/100 [09:44<51:10, 36.55s/it]
generating response:  17%|█▋        | 17/100 [10:20<50:25, 36.45s/it]
generating response:  18%|█▊        | 18/100 [10:57<49:58, 36.57s/it]
generating response:  19%|█▉        | 19/100 [11:34<49:38, 36.77s/it]
generating response:  20%|██        | 20/100 [12:11<49:12, 36.90s/it]
generating response:  21%|██        | 21/100 [12:48<48:25, 36.78s/it]
generating response:  22%|██▏       | 22/100 [13:24<47:49, 36.79s/it]
generating response:  23%|██▎       | 23/100 [14:01<47:10, 36.76s/it]
generating response:  24%|██▍       | 24/100 [14:38<46:30, 36.71s/it]
generating response:  25%|██▌       | 25/100 [15:14<45:48, 36.65s/it]
generating response:  26%|██▌       | 26/100 [15:51<45:14, 36.68s/it]
generating response:  27%|██▋       | 27/100 [16:27<44:32, 36.62s/it]
generating response:  28%|██▊       | 28/100 [17:05<44:20, 36.95s/it]
generating response:  29%|██▉       | 29/100 [17:42<43:35, 36.84s/it]
generating response:  30%|███       | 30/100 [18:19<43:07, 36.96s/it]
generating response:  31%|███       | 31/100 [18:56<42:30, 36.97s/it]
generating response:  32%|███▏      | 32/100 [19:32<41:41, 36.79s/it]
generating response:  33%|███▎      | 33/100 [20:09<41:02, 36.75s/it]
generating response:  34%|███▍      | 34/100 [20:46<40:24, 36.73s/it]
generating response:  35%|███▌      | 35/100 [21:22<39:43, 36.67s/it]
generating response:  36%|███▌      | 36/100 [21:59<39:05, 36.65s/it]
generating response:  37%|███▋      | 37/100 [22:35<38:18, 36.48s/it]
generating response:  38%|███▊      | 38/100 [23:11<37:37, 36.40s/it]
generating response:  39%|███▉      | 39/100 [23:49<37:22, 36.76s/it]
generating response:  40%|████      | 40/100 [24:26<36:47, 36.79s/it]
generating response:  41%|████      | 41/100 [25:01<35:53, 36.50s/it]
generating response:  42%|████▏     | 42/100 [25:38<35:25, 36.65s/it]
generating response:  43%|████▎     | 43/100 [26:14<34:36, 36.43s/it]
generating response:  44%|████▍     | 44/100 [26:51<33:58, 36.41s/it]
generating response:  45%|████▌     | 45/100 [27:27<33:26, 36.49s/it]
generating response:  46%|████▌     | 46/100 [28:03<32:43, 36.36s/it]
generating response:  47%|████▋     | 47/100 [28:40<32:11, 36.44s/it]
generating response:  48%|████▊     | 48/100 [29:16<31:21, 36.17s/it]
generating response:  49%|████▉     | 49/100 [29:51<30:40, 36.08s/it]
generating response:  50%|█████     | 50/100 [30:28<30:17, 36.35s/it]
generating response:  51%|█████     | 51/100 [31:05<29:40, 36.34s/it]
generating response:  52%|█████▏    | 52/100 [31:41<29:04, 36.35s/it]
generating response:  53%|█████▎    | 53/100 [32:18<28:33, 36.46s/it]
generating response:  54%|█████▍    | 54/100 [32:55<28:10, 36.76s/it]
generating response:  55%|█████▌    | 55/100 [33:32<27:34, 36.77s/it]
generating response:  56%|█████▌    | 56/100 [34:08<26:48, 36.57s/it]
generating response:  57%|█████▋    | 57/100 [34:48<26:50, 37.46s/it]
generating response:  58%|█████▊    | 58/100 [35:25<26:07, 37.33s/it]
generating response:  59%|█████▉    | 59/100 [36:01<25:19, 37.06s/it]
generating response:  60%|██████    | 60/100 [36:39<24:48, 37.20s/it]
generating response:  61%|██████    | 61/100 [37:15<23:56, 36.82s/it]
generating response:  62%|██████▏   | 62/100 [37:51<23:18, 36.80s/it]
generating response:  63%|██████▎   | 63/100 [38:28<22:38, 36.72s/it]
generating response:  64%|██████▍   | 64/100 [39:05<22:08, 36.92s/it]
generating response:  65%|██████▌   | 65/100 [39:42<21:28, 36.82s/it]
generating response:  66%|██████▌   | 66/100 [40:19<20:51, 36.80s/it]
generating response:  67%|██████▋   | 67/100 [40:55<20:12, 36.75s/it]
generating response:  68%|██████▊   | 68/100 [41:32<19:32, 36.63s/it]
generating response:  69%|██████▉   | 69/100 [42:08<18:56, 36.65s/it]
generating response:  70%|███████   | 70/100 [42:46<18:25, 36.85s/it]
generating response:  71%|███████   | 71/100 [43:23<17:50, 36.92s/it]
generating response:  72%|███████▏  | 72/100 [43:59<17:05, 36.63s/it]
generating response:  73%|███████▎  | 73/100 [44:35<16:30, 36.67s/it]
generating response:  74%|███████▍  | 74/100 [45:12<15:55, 36.76s/it]
generating response:  75%|███████▌  | 75/100 [45:50<15:21, 36.88s/it]
generating response:  76%|███████▌  | 76/100 [46:26<14:40, 36.67s/it]
generating response:  77%|███████▋  | 77/100 [47:02<14:01, 36.60s/it]
generating response:  78%|███████▊  | 78/100 [47:38<13:20, 36.40s/it]
generating response:  79%|███████▉  | 79/100 [48:14<12:40, 36.23s/it]
generating response:  80%|████████  | 80/100 [48:51<12:08, 36.42s/it]
generating response:  81%|████████  | 81/100 [49:27<11:33, 36.49s/it]
generating response:  82%|████████▏ | 82/100 [50:04<10:54, 36.36s/it]
generating response:  83%|████████▎ | 83/100 [50:40<10:21, 36.53s/it]
generating response:  84%|████████▍ | 84/100 [51:18<09:48, 36.78s/it]
generating response:  85%|████████▌ | 85/100 [51:55<09:12, 36.83s/it]
generating response:  86%|████████▌ | 86/100 [52:31<08:33, 36.71s/it]
generating response:  87%|████████▋ | 87/100 [53:08<07:57, 36.70s/it]
generating response:  88%|████████▊ | 88/100 [53:45<07:20, 36.70s/it]
generating response:  89%|████████▉ | 89/100 [54:23<06:48, 37.15s/it]
generating response:  90%|█████████ | 90/100 [54:59<06:08, 36.85s/it]
generating response:  91%|█████████ | 91/100 [55:35<05:30, 36.69s/it]
generating response:  92%|█████████▏| 92/100 [56:11<04:51, 36.48s/it]
generating response:  93%|█████████▎| 93/100 [56:48<04:16, 36.57s/it]
generating response:  94%|█████████▍| 94/100 [57:25<03:39, 36.64s/it]
generating response:  95%|█████████▌| 95/100 [58:01<03:03, 36.61s/it]
generating response:  96%|█████████▌| 96/100 [58:38<02:26, 36.51s/it]
generating response:  97%|█████████▋| 97/100 [59:14<01:49, 36.49s/it]
generating response:  98%|█████████▊| 98/100 [59:50<01:12, 36.35s/it]
generating response:  99%|█████████▉| 99/100 [1:00:27<00:36, 36.66s/it]
generating response: 100%|██████████| 100/100 [1:01:05<00:00, 36.87s/it]
generating response: 100%|██████████| 100/100 [1:01:05<00:00, 36.65s/it]
generation part takes 3695.00s
Full generation completed in 3815.34s
Traceback (most recent call last):
  File "/home/xuyue/QXYtemp/MLM/./main.py", line 130, in <module>
    new_behaviours.extend([behaviours[i]]*image_num)
AttributeError: 'Namespace' object has no attribute 'not_eval'. Did you mean: 'no_eval'?
