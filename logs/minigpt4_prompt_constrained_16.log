processing images... /home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
denoise batch:   0%|          | 0/1 [00:00<?, ?it/s]denoise batch: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]denoise batch: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]
Done
computing cossim... Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.56it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  3.48it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.95it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
csv file saved at: ./temp_minigpt4_prompt_constrained_16
Done
Defender initialized with threshold=-0.003936767578125
processed 1 images, 160 texts in 18.52s
using minigpt4 for generation
Loading MiniGPT-4 models...You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.08s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]
Loading Q-Former
/home/xuyue/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading Q-Former Done
Load MiniGPT-4 Checkpoint: /home/xuyue/OverallEval/Defense_method/JailGuard-main/JailGuard/checkpoint/pretrained_minigpt4_vicuna13b.pth
Done
generating response:   0%|          | 0/160 [00:00<?, ?it/s]generating response:   1%|          | 1/160 [01:19<3:31:26, 79.79s/it]generating response:  21%|██▏       | 34/160 [02:38<08:22,  3.99s/it] generating response:  26%|██▋       | 42/160 [03:56<10:43,  5.45s/it]generating response:  28%|██▊       | 44/160 [05:16<16:04,  8.32s/it]generating response:  30%|███       | 48/160 [06:36<19:40, 10.54s/it]generating response:  32%|███▏      | 51/160 [07:53<23:51, 13.13s/it]generating response:  35%|███▌      | 56/160 [09:11<23:59, 13.84s/it]generating response:  40%|████      | 64/160 [10:29<19:35, 12.24s/it]generating response:  42%|████▎     | 68/160 [11:47<21:12, 13.83s/it]generating response:  48%|████▊     | 77/160 [13:05<16:10, 11.69s/it]generating response:  57%|█████▋    | 91/160 [14:25<10:07,  8.80s/it]generating response:  59%|█████▉    | 94/160 [15:43<12:06, 11.01s/it]generating response:  59%|█████▉    | 95/160 [17:01<16:07, 14.88s/it]generating response:  63%|██████▎   | 101/160 [18:20<14:03, 14.29s/it]generating response:  66%|██████▋   | 106/160 [19:39<13:16, 14.75s/it]generating response:  70%|███████   | 112/160 [20:59<11:25, 14.29s/it]generating response:  71%|███████   | 113/160 [22:19<14:52, 19.00s/it]generating response:  76%|███████▋  | 122/160 [23:37<08:52, 14.01s/it]generating response:  81%|████████▏ | 130/160 [24:53<06:10, 12.34s/it]generating response:  84%|████████▍ | 134/160 [26:12<06:01, 13.89s/it]generating response:  84%|████████▍ | 135/160 [27:29<07:39, 18.38s/it]generating response:  88%|████████▊ | 140/160 [28:48<05:49, 17.49s/it]generating response:  91%|█████████▏| 146/160 [30:06<03:41, 15.85s/it]generating response:  94%|█████████▍| 150/160 [31:23<02:47, 16.70s/it]generating response: 100%|██████████| 160/160 [31:23<00:00, 11.77s/it]
generation part takes 1918.00s
Full generation completed in 1936.52s
